{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finance Sector Dataset Preparation\n",
    "**Purpose**: Create a clean, feature-rich dataset for training models on finance sector stocks\n",
    "\n",
    "**Date**: 2026-02-26\n",
    "\n",
    "## Overview\n",
    "This notebook prepares the training dataset for the finance sector:\n",
    "- 10 tickers: JPM, BAC, GS, MS, WFC, C, BLK, AXP, USB, PNC\n",
    "- Time period: 2019-2024\n",
    "- Features: 30+ technical and fundamental indicators\n",
    "- Target: 5-day forward returns\n",
    "- Output: Parquet file ready for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date, datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from src.data.data_loader import DataLoader\n",
    "from src.feature_engineering.feature_aggregator import FeatureAggregator\n",
    "from src.feature_engineering.technical_features import TechnicalFeatures\n",
    "from src.feature_engineering.fundamental_features import FundamentalFeatures\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finance sector tickers\n",
    "FINANCE_TICKERS = ['JPM', 'BAC', 'GS', 'MS', 'WFC', 'C', 'BLK', 'AXP', 'USB', 'PNC']\n",
    "\n",
    "# Date ranges\n",
    "START_DATE = date(2019, 1, 1)\n",
    "END_DATE = date(2024, 12, 31)\n",
    "\n",
    "TRAIN_END = date(2023, 12, 31)\n",
    "VAL_START = date(2024, 1, 1)\n",
    "VAL_END = date(2024, 6, 30)\n",
    "TEST_START = date(2024, 7, 1)\n",
    "TEST_END = date(2024, 12, 31)\n",
    "\n",
    "# Target variable\n",
    "FORWARD_DAYS = 5  # Predict 5-day forward returns\n",
    "CLIP_PERCENTILE = 99  # Clip outliers at 99th percentile\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = project_root / 'data' / 'prepared'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nFinance Tickers: {len(FINANCE_TICKERS)}\")\n",
    "print(f\"Date Range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"Target: {FORWARD_DAYS}-day forward returns\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DataLoader()\n",
    "\n",
    "# Load price data for all tickers\n",
    "price_data = {}\n",
    "metadata = {}\n",
    "\n",
    "print(\"Loading price data...\\n\")\n",
    "for ticker in tqdm(FINANCE_TICKERS):\n",
    "    # Load prices\n",
    "    prices = data_loader.load_stock_prices(ticker, START_DATE, END_DATE)\n",
    "    \n",
    "    if not prices.empty:\n",
    "        price_data[ticker] = prices\n",
    "        \n",
    "        # Load metadata\n",
    "        meta = data_loader.load_company_metadata(ticker)\n",
    "        if meta:\n",
    "            metadata[ticker] = meta\n",
    "        \n",
    "        print(f\"âœ“ {ticker}: {len(prices)} records from {prices.index[0].date()} to {prices.index[-1].date()}\")\n",
    "    else:\n",
    "        print(f\"âœ— {ticker}: No data found\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(price_data)} tickers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data completeness\n",
    "quality_report = []\n",
    "\n",
    "for ticker, prices in price_data.items():\n",
    "    n_records = len(prices)\n",
    "    date_range = (prices.index[-1] - prices.index[0]).days\n",
    "    expected_records = date_range * 5 / 7  # Roughly 5 trading days per week\n",
    "    completeness = (n_records / expected_records) * 100\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_close = prices['close'].isna().sum()\n",
    "    missing_volume = prices['volume'].isna().sum()\n",
    "    \n",
    "    quality_report.append({\n",
    "        'ticker': ticker,\n",
    "        'n_records': n_records,\n",
    "        'date_range_days': date_range,\n",
    "        'completeness_pct': completeness,\n",
    "        'missing_close': missing_close,\n",
    "        'missing_volume': missing_volume,\n",
    "        'first_date': prices.index[0].date(),\n",
    "        'last_date': prices.index[-1].date()\n",
    "    })\n",
    "\n",
    "quality_df = pd.DataFrame(quality_report)\n",
    "print(\"\\nData Quality Report:\")\n",
    "print(quality_df.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Average completeness: {quality_df['completeness_pct'].mean():.1f}%\")\n",
    "print(f\"Min records: {quality_df['n_records'].min()}\")\n",
    "print(f\"Max records: {quality_df['n_records'].max()}\")\n",
    "print(f\"Total missing close prices: {quality_df['missing_close'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature aggregator\n",
    "feature_agg = FeatureAggregator()\n",
    "\n",
    "# Compute features for all tickers\n",
    "all_features = []\n",
    "\n",
    "print(\"Computing features...\\n\")\n",
    "for ticker in tqdm(FINANCE_TICKERS):\n",
    "    if ticker not in price_data:\n",
    "        print(f\"Skipping {ticker} - no price data\")\n",
    "        continue\n",
    "    \n",
    "    prices = price_data[ticker]\n",
    "    \n",
    "    # We need to compute features for each date in the dataset\n",
    "    # Use rolling window approach\n",
    "    for i in range(100, len(prices)):  # Start after 100 days for indicator warmup\n",
    "        # Get historical window up to current date\n",
    "        window_prices = prices.iloc[:i+1]\n",
    "        current_date = window_prices.index[-1]\n",
    "        \n",
    "        # Compute all features using the window\n",
    "        features = feature_agg.compute_all_features(\n",
    "            ticker=ticker,\n",
    "            prices=window_prices,\n",
    "            sector_performance=None,  # Can add sector rotation features later\n",
    "            use_cache=True,\n",
    "            save_cache=True\n",
    "        )\n",
    "        \n",
    "        if features:\n",
    "            # Add metadata\n",
    "            features['ticker'] = ticker\n",
    "            features['date'] = current_date\n",
    "            features['close'] = window_prices['close'].iloc[-1]\n",
    "            \n",
    "            all_features.append(features)\n",
    "\n",
    "# Convert to DataFrame\n",
    "features_df = pd.DataFrame(all_features)\n",
    "print(f\"\\nâœ“ Computed features for {len(features_df)} observations\")\n",
    "print(f\"âœ“ Feature columns: {len(features_df.columns)}\")\n",
    "print(f\"\\nSample features:\")\n",
    "print(features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check feature columns\n",
    "print(\"\\nFeature columns:\")\n",
    "feature_cols = [col for col in features_df.columns if col not in ['ticker', 'date', 'close']]\n",
    "print(f\"Total features: {len(feature_cols)}\\n\")\n",
    "for col in sorted(feature_cols):\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate forward returns for each ticker\n",
    "print(f\"Creating {FORWARD_DAYS}-day forward returns...\\n\")\n",
    "\n",
    "features_df = features_df.sort_values(['ticker', 'date'])\n",
    "\n",
    "# Calculate forward returns\n",
    "features_df['forward_return'] = np.nan\n",
    "\n",
    "for ticker in FINANCE_TICKERS:\n",
    "    ticker_mask = features_df['ticker'] == ticker\n",
    "    ticker_data = features_df[ticker_mask].copy()\n",
    "    \n",
    "    # Calculate forward return\n",
    "    ticker_data['forward_return'] = ticker_data['close'].shift(-FORWARD_DAYS) / ticker_data['close'] - 1\n",
    "    \n",
    "    # Update main dataframe\n",
    "    features_df.loc[ticker_mask, 'forward_return'] = ticker_data['forward_return'].values\n",
    "\n",
    "# Remove rows without forward returns (last N days of each ticker)\n",
    "before_drop = len(features_df)\n",
    "features_df = features_df.dropna(subset=['forward_return'])\n",
    "after_drop = len(features_df)\n",
    "\n",
    "print(f\"âœ“ Created forward returns\")\n",
    "print(f\"âœ“ Dropped {before_drop - after_drop} rows without forward returns\")\n",
    "print(f\"âœ“ Remaining observations: {after_drop}\")\n",
    "\n",
    "# Handle outliers (clip at percentiles)\n",
    "lower = features_df['forward_return'].quantile((100 - CLIP_PERCENTILE) / 100)\n",
    "upper = features_df['forward_return'].quantile(CLIP_PERCENTILE / 100)\n",
    "\n",
    "print(f\"\\nReturn statistics (before clipping):\")\n",
    "print(features_df['forward_return'].describe())\n",
    "print(f\"\\nClipping outliers at {CLIP_PERCENTILE}th percentile: [{lower:.4f}, {upper:.4f}]\")\n",
    "\n",
    "features_df['forward_return'] = features_df['forward_return'].clip(lower, upper)\n",
    "\n",
    "print(f\"\\nReturn statistics (after clipping):\")\n",
    "print(features_df['forward_return'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize return distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "features_df['forward_return'].hist(bins=50, edgecolor='black')\n",
    "plt.xlabel(f'{FORWARD_DAYS}-Day Forward Return')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Forward Returns')\n",
    "plt.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for ticker in FINANCE_TICKERS:\n",
    "    ticker_returns = features_df[features_df['ticker'] == ticker]['forward_return']\n",
    "    plt.plot(ticker_returns.index, ticker_returns.cumsum(), label=ticker, alpha=0.7)\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.title('Cumulative Returns by Ticker')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in features\n",
    "missing_counts = features_df.isnull().sum()\n",
    "missing_features = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(\"Features with missing values:\\n\")\n",
    "    for feature, count in missing_features.items():\n",
    "        pct = (count / len(features_df)) * 100\n",
    "        print(f\"  {feature}: {count} ({pct:.2f}%)\")\n",
    "    \n",
    "    # Strategy: Fill missing values with median for numerical features\n",
    "    print(\"\\nFilling missing values with median...\")\n",
    "    for col in missing_features.index:\n",
    "        if col not in ['ticker', 'date']:\n",
    "            median_val = features_df[col].median()\n",
    "            features_df[col] = features_df[col].fillna(median_val)\n",
    "    \n",
    "    print(\"âœ“ Missing values handled\")\n",
    "else:\n",
    "    print(\"âœ“ No missing values in features\")\n",
    "\n",
    "# Final check\n",
    "print(f\"\\nFinal dataset shape: {features_df.shape}\")\n",
    "print(f\"Remaining missing values: {features_df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime if needed\n",
    "if not isinstance(features_df['date'].iloc[0], pd.Timestamp):\n",
    "    features_df['date'] = pd.to_datetime(features_df['date'])\n",
    "\n",
    "# Split by date\n",
    "train_df = features_df[features_df['date'] <= pd.Timestamp(TRAIN_END)].copy()\n",
    "val_df = features_df[\n",
    "    (features_df['date'] >= pd.Timestamp(VAL_START)) & \n",
    "    (features_df['date'] <= pd.Timestamp(VAL_END))\n",
    "].copy()\n",
    "test_df = features_df[\n",
    "    (features_df['date'] >= pd.Timestamp(TEST_START)) & \n",
    "    (features_df['date'] <= pd.Timestamp(TEST_END))\n",
    "].copy()\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  Dates: {train_df['date'].min().date()} to {train_df['date'].max().date()}\")\n",
    "print(f\"  Observations: {len(train_df)}\")\n",
    "print(f\"  Tickers: {train_df['ticker'].nunique()}\")\n",
    "\n",
    "print(f\"\\nValidation set:\")\n",
    "print(f\"  Dates: {val_df['date'].min().date()} to {val_df['date'].max().date()}\")\n",
    "print(f\"  Observations: {len(val_df)}\")\n",
    "print(f\"  Tickers: {val_df['ticker'].nunique()}\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Dates: {test_df['date'].min().date()} to {test_df['date'].max().date()}\")\n",
    "print(f\"  Observations: {len(test_df)}\")\n",
    "print(f\"  Tickers: {test_df['ticker'].nunique()}\")\n",
    "\n",
    "print(f\"\\nTotal: {len(train_df) + len(val_df) + len(test_df)} observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize split\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for ticker in FINANCE_TICKERS[:5]:  # Show first 5 tickers for clarity\n",
    "    ticker_data = features_df[features_df['ticker'] == ticker]\n",
    "    plt.scatter(ticker_data[ticker_data['date'] <= pd.Timestamp(TRAIN_END)]['date'], \n",
    "                [ticker] * len(ticker_data[ticker_data['date'] <= pd.Timestamp(TRAIN_END)]),\n",
    "                alpha=0.3, s=10, color='blue', label='Train' if ticker == FINANCE_TICKERS[0] else '')\n",
    "    plt.scatter(ticker_data[(ticker_data['date'] >= pd.Timestamp(VAL_START)) & \n",
    "                             (ticker_data['date'] <= pd.Timestamp(VAL_END))]['date'],\n",
    "                [ticker] * len(ticker_data[(ticker_data['date'] >= pd.Timestamp(VAL_START)) & \n",
    "                                            (ticker_data['date'] <= pd.Timestamp(VAL_END))]),\n",
    "                alpha=0.3, s=10, color='orange', label='Val' if ticker == FINANCE_TICKERS[0] else '')\n",
    "    plt.scatter(ticker_data[(ticker_data['date'] >= pd.Timestamp(TEST_START)) & \n",
    "                             (ticker_data['date'] <= pd.Timestamp(TEST_END))]['date'],\n",
    "                [ticker] * len(ticker_data[(ticker_data['date'] >= pd.Timestamp(TEST_START)) & \n",
    "                                            (ticker_data['date'] <= pd.Timestamp(TEST_END))]),\n",
    "                alpha=0.3, s=10, color='green', label='Test' if ticker == FINANCE_TICKERS[0] else '')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Ticker')\n",
    "plt.title('Train/Validation/Test Split Visualization')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets to parquet (efficient format)\n",
    "train_path = OUTPUT_DIR / 'finance_train.parquet'\n",
    "val_path = OUTPUT_DIR / 'finance_val.parquet'\n",
    "test_path = OUTPUT_DIR / 'finance_test.parquet'\n",
    "full_path = OUTPUT_DIR / 'finance_full.parquet'\n",
    "\n",
    "print(\"Saving datasets...\\n\")\n",
    "\n",
    "train_df.to_parquet(train_path, index=False)\n",
    "print(f\"âœ“ Training set saved: {train_path}\")\n",
    "print(f\"  Size: {train_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "val_df.to_parquet(val_path, index=False)\n",
    "print(f\"âœ“ Validation set saved: {val_path}\")\n",
    "print(f\"  Size: {val_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "test_df.to_parquet(test_path, index=False)\n",
    "print(f\"âœ“ Test set saved: {test_path}\")\n",
    "print(f\"  Size: {test_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "features_df.to_parquet(full_path, index=False)\n",
    "print(f\"âœ“ Full dataset saved: {full_path}\")\n",
    "print(f\"  Size: {full_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata\n",
    "import json\n",
    "\n",
    "metadata_dict = {\n",
    "    'creation_date': datetime.now().isoformat(),\n",
    "    'sector': 'finance',\n",
    "    'tickers': FINANCE_TICKERS,\n",
    "    'n_tickers': len(FINANCE_TICKERS),\n",
    "    'date_range': {\n",
    "        'start': START_DATE.isoformat(),\n",
    "        'end': END_DATE.isoformat()\n",
    "    },\n",
    "    'splits': {\n",
    "        'train': {\n",
    "            'end_date': TRAIN_END.isoformat(),\n",
    "            'n_observations': len(train_df)\n",
    "        },\n",
    "        'validation': {\n",
    "            'start_date': VAL_START.isoformat(),\n",
    "            'end_date': VAL_END.isoformat(),\n",
    "            'n_observations': len(val_df)\n",
    "        },\n",
    "        'test': {\n",
    "            'start_date': TEST_START.isoformat(),\n",
    "            'end_date': TEST_END.isoformat(),\n",
    "            'n_observations': len(test_df)\n",
    "        }\n",
    "    },\n",
    "    'target': {\n",
    "        'name': 'forward_return',\n",
    "        'forward_days': FORWARD_DAYS,\n",
    "        'clip_percentile': CLIP_PERCENTILE\n",
    "    },\n",
    "    'features': {\n",
    "        'n_features': len(feature_cols),\n",
    "        'feature_names': feature_cols\n",
    "    },\n",
    "    'statistics': {\n",
    "        'total_observations': len(features_df),\n",
    "        'train_observations': len(train_df),\n",
    "        'val_observations': len(val_df),\n",
    "        'test_observations': len(test_df)\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = OUTPUT_DIR / 'finance_dataset_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata_dict, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Metadata saved: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINANCE SECTOR DATASET SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset Statistics:\")\n",
    "print(f\"  Total observations: {len(features_df):,}\")\n",
    "print(f\"  Training: {len(train_df):,} ({len(train_df)/len(features_df)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(val_df):,} ({len(val_df)/len(features_df)*100:.1f}%)\")\n",
    "print(f\"  Test: {len(test_df):,} ({len(test_df)/len(features_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ¦ Tickers: {len(FINANCE_TICKERS)}\")\n",
    "print(f\"  {', '.join(FINANCE_TICKERS)}\")\n",
    "\n",
    "print(f\"\\nðŸ“… Date Range:\")\n",
    "print(f\"  Overall: {START_DATE} to {END_DATE}\")\n",
    "print(f\"  Training: up to {TRAIN_END}\")\n",
    "print(f\"  Validation: {VAL_START} to {VAL_END}\")\n",
    "print(f\"  Test: {TEST_START} to {TEST_END}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Target Variable:\")\n",
    "print(f\"  Name: forward_return\")\n",
    "print(f\"  Period: {FORWARD_DAYS} days\")\n",
    "print(f\"  Mean: {features_df['forward_return'].mean():.4f}\")\n",
    "print(f\"  Std: {features_df['forward_return'].std():.4f}\")\n",
    "print(f\"  Min: {features_df['forward_return'].min():.4f}\")\n",
    "print(f\"  Max: {features_df['forward_return'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ”§ Features: {len(feature_cols)}\")\n",
    "print(f\"  Technical indicators: ~17\")\n",
    "print(f\"  Fundamental features: ~15+\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ Output Files:\")\n",
    "print(f\"  {train_path.name}\")\n",
    "print(f\"  {val_path.name}\")\n",
    "print(f\"  {test_path.name}\")\n",
    "print(f\"  {full_path.name}\")\n",
    "print(f\"  {metadata_path.name}\")\n",
    "\n",
    "print(f\"\\nâœ… Dataset preparation complete!\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Train linear model: Create 03_train_linear_model.ipynb\")\n",
    "print(f\"  2. Load dataset: pd.read_parquet('{train_path}')\")\n",
    "print(f\"  3. Train and evaluate models\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
