{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Model Training\n",
    "**Purpose**: Train tree-based and ensemble models to capture nonlinear patterns in financial data\n",
    "\n",
    "## Pipeline\n",
    "1. Load prepared dataset & selected features from 03a\n",
    "2. Train Random Forest, Gradient Boosting, and XGBoost\n",
    "3. Hyperparameter tuning via validation set\n",
    "4. Feature importance analysis\n",
    "5. Compare all models (including linear baseline)\n",
    "6. Save best model & artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('✓ Libraries loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "DATA_DIR = Path('/home/archy/Desktop/Server/FinancialData/model_regime_comparison/data/prepared')\n",
    "MODEL_DIR = Path('/home/archy/Desktop/Server/FinancialData/model_regime_comparison/models')\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TARGET = 'forward_return_30d'\n",
    "EXCLUDE_COLS = ['ticker', 'date', 'close', 'forward_return_5d', 'forward_return_30d']\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f'Data dir: {DATA_DIR}')\n",
    "print(f'Target: {TARGET}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data & Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(DATA_DIR / 'finance_train.parquet')\n",
    "val_df = pd.read_parquet(DATA_DIR / 'finance_val.parquet')\n",
    "test_df = pd.read_parquet(DATA_DIR / 'finance_test.parquet')\n",
    "\n",
    "print(f'Train: {train_df.shape}')\n",
    "print(f'Val:   {val_df.shape}')\n",
    "print(f'Test:  {test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load selected features from 03a (if available), otherwise use all numeric features\n",
    "features_path = MODEL_DIR / 'selected_features.json'\n",
    "if features_path.exists():\n",
    "    with open(features_path) as f:\n",
    "        feat_info = json.load(f)\n",
    "    selected_features = feat_info['selected_features']\n",
    "    print(f'Loaded {len(selected_features)} selected features from 03a')\n",
    "else:\n",
    "    all_feature_cols = [c for c in train_df.columns if c not in EXCLUDE_COLS]\n",
    "    selected_features = train_df[all_feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    print(f'No saved features found, using all {len(selected_features)} numeric features')\n",
    "\n",
    "print(f'Features: {selected_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target\n",
    "X_train = train_df[selected_features].copy()\n",
    "y_train = train_df[TARGET].copy()\n",
    "X_val = val_df[selected_features].copy()\n",
    "y_val = val_df[TARGET].copy()\n",
    "X_test = test_df[selected_features].copy()\n",
    "y_test = test_df[TARGET].copy()\n",
    "\n",
    "# Fill missing values with training median\n",
    "train_medians = X_train.median()\n",
    "X_train = X_train.fillna(train_medians)\n",
    "X_val = X_val.fillna(train_medians)\n",
    "X_test = X_test.fillna(train_medians)\n",
    "\n",
    "# Drop rows where target is NaN\n",
    "train_mask = ~y_train.isna()\n",
    "val_mask = ~y_val.isna()\n",
    "test_mask = ~y_test.isna()\n",
    "X_train, y_train = X_train[train_mask], y_train[train_mask]\n",
    "X_val, y_val = X_val[val_mask], y_val[val_mask]\n",
    "X_test, y_test = X_test[test_mask], y_test[test_mask]\n",
    "\n",
    "# Drop any remaining NaN columns\n",
    "still_nan = X_train.columns[X_train.isnull().any()].tolist()\n",
    "if still_nan:\n",
    "    print(f'Dropping {len(still_nan)} columns still containing NaN: {still_nan}')\n",
    "    X_train = X_train.drop(columns=still_nan)\n",
    "    X_val = X_val.drop(columns=still_nan)\n",
    "    X_test = X_test.drop(columns=still_nan)\n",
    "    selected_features = [c for c in selected_features if c not in still_nan]\n",
    "\n",
    "print(f'Train: {X_train.shape}')\n",
    "print(f'Val:   {X_val.shape}')\n",
    "print(f'Test:  {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Nonlinear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_tr, y_tr, X_v, y_v):\n",
    "    \"\"\"Train and evaluate a model, return metrics dict.\"\"\"\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_tr_pred = model.predict(X_tr)\n",
    "    y_v_pred = model.predict(X_v)\n",
    "    return {\n",
    "        'Train RMSE': np.sqrt(mean_squared_error(y_tr, y_tr_pred)),\n",
    "        'Val RMSE': np.sqrt(mean_squared_error(y_v, y_v_pred)),\n",
    "        'Train MAE': mean_absolute_error(y_tr, y_tr_pred),\n",
    "        'Val MAE': mean_absolute_error(y_v, y_v_pred),\n",
    "        'Train R²': r2_score(y_tr, y_tr_pred),\n",
    "        'Val R²': r2_score(y_v, y_v_pred),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models with varying hyperparameters\n",
    "models = {\n",
    "    # Random Forest variants\n",
    "    'RF (100, d=5)': RandomForestRegressor(\n",
    "        n_estimators=100, max_depth=5, min_samples_leaf=20,\n",
    "        random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    'RF (200, d=8)': RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=8, min_samples_leaf=10,\n",
    "        random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    'RF (300, d=12)': RandomForestRegressor(\n",
    "        n_estimators=300, max_depth=12, min_samples_leaf=5,\n",
    "        random_state=RANDOM_STATE, n_jobs=-1),\n",
    "\n",
    "    # Gradient Boosting variants\n",
    "    'GBR (100, d=3, lr=0.1)': GradientBoostingRegressor(\n",
    "        n_estimators=100, max_depth=3, learning_rate=0.1,\n",
    "        subsample=0.8, random_state=RANDOM_STATE),\n",
    "    'GBR (200, d=4, lr=0.05)': GradientBoostingRegressor(\n",
    "        n_estimators=200, max_depth=4, learning_rate=0.05,\n",
    "        subsample=0.8, random_state=RANDOM_STATE),\n",
    "    'GBR (500, d=3, lr=0.01)': GradientBoostingRegressor(\n",
    "        n_estimators=500, max_depth=3, learning_rate=0.01,\n",
    "        subsample=0.8, random_state=RANDOM_STATE),\n",
    "\n",
    "    # XGBoost variants\n",
    "    'XGB (100, d=3, lr=0.1)': xgb.XGBRegressor(\n",
    "        n_estimators=100, max_depth=3, learning_rate=0.1,\n",
    "        subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1.0,\n",
    "        random_state=RANDOM_STATE, n_jobs=-1, verbosity=0),\n",
    "    'XGB (200, d=4, lr=0.05)': xgb.XGBRegressor(\n",
    "        n_estimators=200, max_depth=4, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1.0,\n",
    "        random_state=RANDOM_STATE, n_jobs=-1, verbosity=0),\n",
    "    'XGB (500, d=3, lr=0.01)': xgb.XGBRegressor(\n",
    "        n_estimators=500, max_depth=3, learning_rate=0.01,\n",
    "        subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1.0,\n",
    "        random_state=RANDOM_STATE, n_jobs=-1, verbosity=0),\n",
    "    'XGB (1000, d=4, lr=0.005)': xgb.XGBRegressor(\n",
    "        n_estimators=1000, max_depth=4, learning_rate=0.005,\n",
    "        subsample=0.8, colsample_bytree=0.8, reg_alpha=0.5, reg_lambda=2.0,\n",
    "        random_state=RANDOM_STATE, n_jobs=-1, verbosity=0),\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    metrics = evaluate_model(model, X_train, y_train, X_val, y_val)\n",
    "    metrics['Model'] = name\n",
    "    metrics['model_obj'] = model\n",
    "    results.append(metrics)\n",
    "    print(f'{name:32s}  Train RMSE={metrics[\"Train RMSE\"]:.6f}  Val RMSE={metrics[\"Val RMSE\"]:.6f}  Val R²={metrics[\"Val R²\"]:.6f}')\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print('\\n✓ All models trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by validation RMSE\n",
    "display_df = results_df[['Model', 'Train RMSE', 'Val RMSE', 'Train MAE', 'Val MAE', 'Train R²', 'Val R²']]\n",
    "display_df = display_df.sort_values('Val RMSE')\n",
    "print(display_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
    "sorted_df = results_df.sort_values('Val RMSE')\n",
    "x = np.arange(len(sorted_df))\n",
    "w = 0.35\n",
    "\n",
    "for ax, metric, title in zip(axes,\n",
    "    [('Train RMSE', 'Val RMSE'), ('Train MAE', 'Val MAE'), ('Train R²', 'Val R²')],\n",
    "    ['RMSE', 'MAE', 'R² Score']):\n",
    "    ax.bar(x - w/2, sorted_df[metric[0]], w, label='Train', alpha=0.8)\n",
    "    ax.bar(x + w/2, sorted_df[metric[1]], w, label='Validation', alpha=0.8)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(sorted_df['Model'], rotation=55, ha='right', fontsize=8)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting analysis: train vs val gap\n",
    "sorted_df = results_df.sort_values('Val RMSE').copy()\n",
    "sorted_df['Overfit Gap'] = sorted_df['Train RMSE'] - sorted_df['Val RMSE']\n",
    "sorted_df['Overfit Ratio'] = sorted_df['Val RMSE'] / sorted_df['Train RMSE']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(sorted_df))\n",
    "ax.bar(x, sorted_df['Overfit Ratio'], color='coral', alpha=0.7)\n",
    "ax.axhline(1.0, color='green', linestyle='--', lw=1.5, label='No overfit (ratio=1.0)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(sorted_df['Model'], rotation=55, ha='right', fontsize=8)\n",
    "ax.set_ylabel('Val RMSE / Train RMSE')\n",
    "ax.set_title('Overfitting Analysis (closer to 1.0 = less overfit)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Best Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model by validation RMSE\n",
    "best_idx = results_df['Val RMSE'].idxmin()\n",
    "best_name = results_df.loc[best_idx, 'Model']\n",
    "best_model = results_df.loc[best_idx, 'model_obj']\n",
    "\n",
    "print(f'Best Model: {best_name}')\n",
    "print(f'   Val RMSE: {results_df.loc[best_idx, \"Val RMSE\"]:.6f}')\n",
    "print(f'   Val R²:   {results_df.loc[best_idx, \"Val R²\"]:.6f}')\n",
    "\n",
    "# Test set evaluation\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f'\\n   Test RMSE: {test_rmse:.6f}')\n",
    "print(f'   Test MAE:  {test_mae:.6f}')\n",
    "print(f'   Test R²:   {test_r2:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction vs actual and residuals\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Predicted vs actual\n",
    "axes[0].scatter(y_val, y_val_pred, alpha=0.3, s=10)\n",
    "lims = [min(y_val.min(), y_val_pred.min()), max(y_val.max(), y_val_pred.max())]\n",
    "axes[0].plot(lims, lims, 'r--', lw=1.5)\n",
    "axes[0].set_xlabel('Actual')\n",
    "axes[0].set_ylabel('Predicted')\n",
    "axes[0].set_title(f'Validation: Predicted vs Actual')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_val - y_val_pred\n",
    "axes[1].scatter(y_val_pred, residuals, alpha=0.3, s=10)\n",
    "axes[1].axhline(0, color='red', linestyle='--', lw=1.5)\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Residual')\n",
    "axes[1].set_title('Residual Plot')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual distribution\n",
    "axes[2].hist(residuals, bins=40, edgecolor='black', alpha=0.7)\n",
    "axes[2].axvline(0, color='red', linestyle='--', lw=1.5)\n",
    "axes[2].set_xlabel('Residual')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_title(f'Residual Distribution (μ={residuals.mean():.4f}, σ={residuals.std():.4f})')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from best model\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    imp = best_model.feature_importances_\n",
    "    imp_df = pd.DataFrame({\n",
    "        'Feature': selected_features,\n",
    "        'Importance': imp\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, max(6, len(imp_df) * 0.35)))\n",
    "    ax.barh(range(len(imp_df)), imp_df['Importance'], color='teal', alpha=0.7)\n",
    "    ax.set_yticks(range(len(imp_df)))\n",
    "    ax.set_yticklabels(imp_df['Feature'])\n",
    "    ax.set_xlabel('Feature Importance')\n",
    "    ax.set_title(f'Feature Importance - {best_name}')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(imp_df.to_string(index=False))\n",
    "else:\n",
    "    print('Best model does not expose feature_importances_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature importance across model families\n",
    "# Pick the best from each family\n",
    "families = {'RF': 'RF', 'GBR': 'GBR', 'XGB': 'XGB'}\n",
    "family_best = {}\n",
    "for prefix, label in families.items():\n",
    "    family_rows = results_df[results_df['Model'].str.startswith(prefix)]\n",
    "    if len(family_rows) > 0:\n",
    "        best_family_idx = family_rows['Val RMSE'].idxmin()\n",
    "        family_best[label] = family_rows.loc[best_family_idx, 'model_obj']\n",
    "\n",
    "if len(family_best) > 1:\n",
    "    fig, axes = plt.subplots(1, len(family_best), figsize=(7 * len(family_best), max(6, len(selected_features) * 0.35)))\n",
    "    if len(family_best) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (label, model) in zip(axes, family_best.items()):\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            imp = model.feature_importances_\n",
    "            imp_df = pd.DataFrame({'Feature': selected_features, 'Importance': imp}).sort_values('Importance', ascending=True)\n",
    "            ax.barh(range(len(imp_df)), imp_df['Importance'], alpha=0.7)\n",
    "            ax.set_yticks(range(len(imp_df)))\n",
    "            ax.set_yticklabels(imp_df['Feature'])\n",
    "            ax.set_xlabel('Importance')\n",
    "            ax.set_title(f'{label} (best)')\n",
    "            ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    plt.suptitle('Feature Importance Comparison Across Model Families', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare with Linear Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load linear model results if available\n",
    "meta_path = MODEL_DIR / 'model_metadata.json'\n",
    "if meta_path.exists():\n",
    "    with open(meta_path) as f:\n",
    "        linear_meta = json.load(f)\n",
    "    print('Linear baseline (from 03a):')\n",
    "    print(f'  Model:     {linear_meta[\"model_name\"]}')\n",
    "    print(f'  Val RMSE:  {linear_meta[\"val_rmse\"]:.6f}')\n",
    "    print(f'  Val R²:    {linear_meta[\"val_r2\"]:.6f}')\n",
    "    print(f'  Test RMSE: {linear_meta[\"test_rmse\"]:.6f}')\n",
    "    print(f'  Test R²:   {linear_meta[\"test_r2\"]:.6f}')\n",
    "    print()\n",
    "\n",
    "print(f'Best nonlinear model ({best_name}):')\n",
    "print(f'  Val RMSE:  {results_df.loc[best_idx, \"Val RMSE\"]:.6f}')\n",
    "print(f'  Val R²:    {results_df.loc[best_idx, \"Val R²\"]:.6f}')\n",
    "print(f'  Test RMSE: {test_rmse:.6f}')\n",
    "print(f'  Test R²:   {test_r2:.6f}')\n",
    "\n",
    "if meta_path.exists():\n",
    "    rmse_improvement = (linear_meta['test_rmse'] - test_rmse) / linear_meta['test_rmse'] * 100\n",
    "    print(f'\\nTest RMSE improvement over linear: {rmse_improvement:+.2f}%')\n",
    "    r2_diff = test_r2 - linear_meta['test_r2']\n",
    "    print(f'Test R² improvement over linear: {r2_diff:+.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Best Model & Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = MODEL_DIR / 'best_nonlinear_model.pkl'\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f'✓ Model saved to {model_path}')\n",
    "\n",
    "# Save metadata\n",
    "nl_metadata = {\n",
    "    'model_name': best_name,\n",
    "    'model_type': 'nonlinear',\n",
    "    'target': TARGET,\n",
    "    'n_features': len(selected_features),\n",
    "    'features': selected_features,\n",
    "    'train_size': len(X_train),\n",
    "    'val_rmse': float(results_df.loc[best_idx, 'Val RMSE']),\n",
    "    'val_r2': float(results_df.loc[best_idx, 'Val R²']),\n",
    "    'test_rmse': float(test_rmse),\n",
    "    'test_mae': float(test_mae),\n",
    "    'test_r2': float(test_r2),\n",
    "    'all_results': [\n",
    "        {k: v for k, v in row.items() if k != 'model_obj'}\n",
    "        for _, row in results_df.iterrows()\n",
    "    ]\n",
    "}\n",
    "meta_path = MODEL_DIR / 'nonlinear_model_metadata.json'\n",
    "with open(meta_path, 'w') as f:\n",
    "    json.dump(nl_metadata, f, indent=2)\n",
    "print(f'✓ Metadata saved to {meta_path}')\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'Best Model: {best_name}')\n",
    "print(f'Features: {len(selected_features)}')\n",
    "print(f'Val RMSE:  {results_df.loc[best_idx, \"Val RMSE\"]:.6f}')\n",
    "print(f'Val R²:    {results_df.loc[best_idx, \"Val R²\"]:.6f}')\n",
    "print(f'Test RMSE: {test_rmse:.6f}')\n",
    "print(f'Test R²:   {test_r2:.6f}')\n",
    "print(f'{\"=\"*60}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
