{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regime-Conditioned Model Training\n",
    "**Purpose**: Test whether training separate models per market regime outperforms a single global model\n",
    "\n",
    "## Hypothesis\n",
    "Standard features may have no predictive power globally but could be useful within specific\n",
    "market regimes (bull/bear/sideways). Regime-conditioned models train on regime-specific subsets\n",
    "of the data and are selected at inference time based on the detected regime.\n",
    "\n",
    "## Pipeline\n",
    "1. Load data & detect regimes using HMM\n",
    "2. Analyze regime distribution and characteristics\n",
    "3. Train per-regime models (XGBoost)\n",
    "4. Build regime-switching ensemble\n",
    "5. Compare: global model vs regime-conditioned vs naive baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# Add project root for HMM detector\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "from src.regime.hmm_detector import HMMRegimeDetector\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('✓ Libraries loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "DATA_DIR = Path('/home/archy/Desktop/Server/FinancialData/model_regime_comparison/data/prepared')\n",
    "MODEL_DIR = Path('/home/archy/Desktop/Server/FinancialData/model_regime_comparison/models')\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TARGET = 'forward_return_30d'\n",
    "EXCLUDE_COLS = ['ticker', 'date', 'close', 'forward_return_5d', 'forward_return_30d']\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Minimum samples per regime to train a dedicated model\n",
    "MIN_REGIME_SAMPLES = 100\n",
    "\n",
    "print(f'Target: {TARGET}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(DATA_DIR / 'finance_train.parquet')\n",
    "val_df = pd.read_parquet(DATA_DIR / 'finance_val.parquet')\n",
    "test_df = pd.read_parquet(DATA_DIR / 'finance_test.parquet')\n",
    "\n",
    "# Load selected features\n",
    "features_path = MODEL_DIR / 'selected_features.json'\n",
    "if features_path.exists():\n",
    "    with open(features_path) as f:\n",
    "        feat_info = json.load(f)\n",
    "    selected_features = feat_info['selected_features']\n",
    "    print(f'Loaded {len(selected_features)} selected features from 03a')\n",
    "else:\n",
    "    all_feature_cols = [c for c in train_df.columns if c not in EXCLUDE_COLS]\n",
    "    selected_features = train_df[all_feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    print(f'Using all {len(selected_features)} numeric features')\n",
    "\n",
    "print(f'Train: {train_df.shape}, Val: {val_df.shape}, Test: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regime Detection\n",
    "Train HMM on per-ticker close prices to assign bull/bear/sideways labels to each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn import hmm as hmmlearn_hmm\n",
    "\n",
    "def _fit_hmm_safe(prices, n_states=3, window=20, random_state=42):\n",
    "    \"\"\"Fit HMM with diagonal covariance (more stable) and return regime history.\"\"\"\n",
    "    returns = prices.pct_change().dropna()\n",
    "    volatility = returns.rolling(window=window).std().dropna()\n",
    "    aligned_returns = returns[volatility.index]\n",
    "    features = np.column_stack([aligned_returns.values, volatility.values])\n",
    "    dates = prices.index[window:]\n",
    "\n",
    "    model = hmmlearn_hmm.GaussianHMM(\n",
    "        n_components=n_states, covariance_type='diag',\n",
    "        n_iter=100, random_state=random_state)\n",
    "    model.fit(features)\n",
    "\n",
    "    states = model.predict(features)\n",
    "    probs = model.predict_proba(features)\n",
    "\n",
    "    # Label states by mean return\n",
    "    state_means = [features[states == s, 0].mean() for s in range(n_states)]\n",
    "    sorted_idx = np.argsort(state_means)\n",
    "    labels = {sorted_idx[0]: 'bear', sorted_idx[1]: 'sideways', sorted_idx[2]: 'bull'}\n",
    "\n",
    "    records = []\n",
    "    for i, (dt, si) in enumerate(zip(dates, states)):\n",
    "        records.append({'date': dt, 'regime': labels[si], 'confidence': probs[i, si]})\n",
    "    return pd.DataFrame(records).set_index('date')\n",
    "\n",
    "def assign_regimes(df, window=20):\n",
    "    \"\"\"\n",
    "    Assign regime labels to each row by fitting an HMM per ticker on close prices.\n",
    "    Returns df with added 'regime' and 'regime_confidence' columns.\n",
    "    \"\"\"\n",
    "    regime_records = []\n",
    "\n",
    "    for ticker in df['ticker'].unique():\n",
    "        ticker_df = df[df['ticker'] == ticker].sort_values('date')\n",
    "        prices = ticker_df.set_index('date')['close']\n",
    "\n",
    "        if len(prices) < window + 50:\n",
    "            print(f'  {ticker}: skipping (only {len(prices)} rows)')\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            history = _fit_hmm_safe(prices, n_states=3, window=window, random_state=RANDOM_STATE)\n",
    "        except Exception as e:\n",
    "            print(f'  {ticker}: HMM failed ({e}), skipping')\n",
    "            continue\n",
    "\n",
    "        for date_idx, row in history.iterrows():\n",
    "            regime_records.append({\n",
    "                'ticker': ticker,\n",
    "                'date': date_idx,\n",
    "                'regime': row['regime'],\n",
    "                'regime_confidence': row['confidence']\n",
    "            })\n",
    "\n",
    "    regime_df = pd.DataFrame(regime_records)\n",
    "    df = df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    regime_df['date'] = pd.to_datetime(regime_df['date'])\n",
    "    merged = df.merge(regime_df, on=['ticker', 'date'], how='left')\n",
    "    return merged\n",
    "\n",
    "print('Assigning regimes to training set...')\n",
    "train_df = assign_regimes(train_df)\n",
    "print(f'  Rows with regime: {train_df[\"regime\"].notna().sum()} / {len(train_df)}')\n",
    "\n",
    "print('\\nAssigning regimes to validation set...')\n",
    "val_df = assign_regimes(val_df)\n",
    "print(f'  Rows with regime: {val_df[\"regime\"].notna().sum()} / {len(val_df)}')\n",
    "\n",
    "print('\\nAssigning regimes to test set...')\n",
    "test_df = assign_regimes(test_df)\n",
    "print(f'  Rows with regime: {test_df[\"regime\"].notna().sum()} / {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows without regime assignment\n",
    "train_df = train_df.dropna(subset=['regime'])\n",
    "val_df = val_df.dropna(subset=['regime'])\n",
    "test_df = test_df.dropna(subset=['regime'])\n",
    "\n",
    "print('After dropping rows without regime:')\n",
    "print(f'  Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regime Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regime distribution across splits\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "regime_order = ['bull', 'sideways', 'bear']\n",
    "colors = {'bull': 'green', 'sideways': 'gray', 'bear': 'red'}\n",
    "\n",
    "for ax, (name, df) in zip(axes, [('Train', train_df), ('Validation', val_df), ('Test', test_df)]):\n",
    "    counts = df['regime'].value_counts()\n",
    "    bars = ax.bar([r for r in regime_order if r in counts.index],\n",
    "                  [counts.get(r, 0) for r in regime_order if r in counts.index],\n",
    "                  color=[colors[r] for r in regime_order if r in counts.index], alpha=0.7)\n",
    "    ax.set_title(f'{name} ({len(df)} rows)')\n",
    "    ax.set_ylabel('Count')\n",
    "    for bar, r in zip(bars, [r for r in regime_order if r in counts.index]):\n",
    "        pct = counts.get(r, 0) / len(df) * 100\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "                f'{pct:.1f}%', ha='center', va='bottom', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Regime Distribution Across Data Splits', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution per regime\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for ax, regime in zip(axes, regime_order):\n",
    "    regime_data = train_df[train_df['regime'] == regime][TARGET].dropna()\n",
    "    if len(regime_data) > 0:\n",
    "        ax.hist(regime_data, bins=40, edgecolor='black', alpha=0.7, color=colors[regime])\n",
    "        ax.axvline(regime_data.mean(), color='black', linestyle='--', lw=2,\n",
    "                   label=f'mean={regime_data.mean():.4f}')\n",
    "        ax.axvline(0, color='gray', linestyle=':', lw=1)\n",
    "        ax.set_title(f'{regime.upper()} (n={len(regime_data)}, σ={regime_data.std():.4f})')\n",
    "        ax.set_xlabel(TARGET)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Target Distribution by Regime (Train)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary stats\n",
    "print(f'{\"Regime\":<12s} {\"Count\":>6s} {\"Mean\":>10s} {\"Std\":>10s} {\"Median\":>10s}')\n",
    "print('-' * 50)\n",
    "for regime in regime_order:\n",
    "    rd = train_df[train_df['regime'] == regime][TARGET].dropna()\n",
    "    if len(rd) > 0:\n",
    "        print(f'{regime:<12s} {len(rd):>6d} {rd.mean():>10.4f} {rd.std():>10.4f} {rd.median():>10.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_Xy(df, features, target):\n",
    "    \"\"\"Extract X, y from df, fill NaN, drop target NaN rows.\"\"\"\n",
    "    X = df[features].copy()\n",
    "    y = df[target].copy()\n",
    "    train_medians = X.median()\n",
    "    X = X.fillna(train_medians)\n",
    "    # Drop remaining NaN columns\n",
    "    still_nan = X.columns[X.isnull().any()].tolist()\n",
    "    if still_nan:\n",
    "        X = X.drop(columns=still_nan)\n",
    "    mask = ~y.isna()\n",
    "    return X[mask], y[mask], df[mask]\n",
    "\n",
    "X_train, y_train, train_meta = prepare_Xy(train_df, selected_features, TARGET)\n",
    "X_val, y_val, val_meta = prepare_Xy(val_df, selected_features, TARGET)\n",
    "X_test, y_test, test_meta = prepare_Xy(test_df, selected_features, TARGET)\n",
    "\n",
    "# Use consistent feature set\n",
    "common_features = list(X_train.columns)\n",
    "X_val = X_val[common_features]\n",
    "X_test = X_test[common_features]\n",
    "\n",
    "print(f'Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Global Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best XGBoost config from notebook 04 (low learning rate, moderate depth)\n",
    "global_model = xgb.XGBRegressor(\n",
    "    n_estimators=500, max_depth=3, learning_rate=0.01,\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE, n_jobs=-1, verbosity=0\n",
    ")\n",
    "global_model.fit(X_train, y_train)\n",
    "\n",
    "global_val_pred = global_model.predict(X_val)\n",
    "global_test_pred = global_model.predict(X_test)\n",
    "\n",
    "print('Global XGBoost Baseline:')\n",
    "print(f'  Val  RMSE={np.sqrt(mean_squared_error(y_val, global_val_pred)):.6f}  R²={r2_score(y_val, global_val_pred):.6f}')\n",
    "print(f'  Test RMSE={np.sqrt(mean_squared_error(y_test, global_test_pred)):.6f}  R²={r2_score(y_test, global_test_pred):.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Per-Regime Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regime_models = {}\n",
    "regime_results = []\n",
    "\n",
    "for regime in regime_order:\n",
    "    # Get regime subsets\n",
    "    train_mask = train_meta['regime'].values == regime\n",
    "    val_mask = val_meta['regime'].values == regime\n",
    "\n",
    "    X_tr_r = X_train[train_mask]\n",
    "    y_tr_r = y_train[train_mask]\n",
    "    X_v_r = X_val[val_mask]\n",
    "    y_v_r = y_val[val_mask]\n",
    "\n",
    "    print(f'\\n{regime.upper()} regime: train={len(X_tr_r)}, val={len(X_v_r)}')\n",
    "\n",
    "    if len(X_tr_r) < MIN_REGIME_SAMPLES:\n",
    "        print(f'  ⚠ Too few training samples, using global model as fallback')\n",
    "        regime_models[regime] = global_model\n",
    "        continue\n",
    "\n",
    "    # Train regime-specific model\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=500, max_depth=3, learning_rate=0.01,\n",
    "        subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1.0,\n",
    "        random_state=RANDOM_STATE, n_jobs=-1, verbosity=0\n",
    "    )\n",
    "    model.fit(X_tr_r, y_tr_r)\n",
    "    regime_models[regime] = model\n",
    "\n",
    "    # Evaluate on regime subset\n",
    "    y_tr_pred = model.predict(X_tr_r)\n",
    "    tr_rmse = np.sqrt(mean_squared_error(y_tr_r, y_tr_pred))\n",
    "    tr_r2 = r2_score(y_tr_r, y_tr_pred)\n",
    "\n",
    "    if len(X_v_r) > 0:\n",
    "        y_v_pred = model.predict(X_v_r)\n",
    "        v_rmse = np.sqrt(mean_squared_error(y_v_r, y_v_pred))\n",
    "        v_r2 = r2_score(y_v_r, y_v_pred)\n",
    "        print(f'  Train RMSE={tr_rmse:.6f}  R²={tr_r2:.6f}')\n",
    "        print(f'  Val   RMSE={v_rmse:.6f}  R²={v_r2:.6f}')\n",
    "    else:\n",
    "        v_rmse, v_r2 = np.nan, np.nan\n",
    "        print(f'  Train RMSE={tr_rmse:.6f}  R²={tr_r2:.6f}')\n",
    "        print(f'  No validation samples for this regime')\n",
    "\n",
    "    regime_results.append({\n",
    "        'Regime': regime, 'Train Samples': len(X_tr_r), 'Val Samples': len(X_v_r),\n",
    "        'Train RMSE': tr_rmse, 'Train R²': tr_r2,\n",
    "        'Val RMSE': v_rmse, 'Val R²': v_r2\n",
    "    })\n",
    "\n",
    "print('\\n✓ Per-regime models trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regime-Switching Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regime_switching_predict(X, regime_labels, regime_models, fallback_model):\n",
    "    \"\"\"\n",
    "    Generate predictions by selecting the appropriate model for each row's regime.\n",
    "    \"\"\"\n",
    "    predictions = np.zeros(len(X))\n",
    "    for regime, model in regime_models.items():\n",
    "        mask = regime_labels == regime\n",
    "        if mask.any():\n",
    "            predictions[mask] = model.predict(X[mask])\n",
    "    # Fallback for any unassigned rows\n",
    "    unassigned = ~np.isin(regime_labels, list(regime_models.keys()))\n",
    "    if unassigned.any():\n",
    "        predictions[unassigned] = fallback_model.predict(X[unassigned])\n",
    "    return predictions\n",
    "\n",
    "# Validation predictions\n",
    "val_regimes = val_meta['regime'].values\n",
    "regime_val_pred = regime_switching_predict(X_val, val_regimes, regime_models, global_model)\n",
    "\n",
    "# Test predictions\n",
    "test_regimes = test_meta['regime'].values\n",
    "regime_test_pred = regime_switching_predict(X_test, test_regimes, regime_models, global_model)\n",
    "\n",
    "print('Regime-Switching Ensemble:')\n",
    "print(f'  Val  RMSE={np.sqrt(mean_squared_error(y_val, regime_val_pred)):.6f}  R²={r2_score(y_val, regime_val_pred):.6f}')\n",
    "print(f'  Test RMSE={np.sqrt(mean_squared_error(y_test, regime_test_pred)):.6f}  R²={r2_score(y_test, regime_test_pred):.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Full Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive baseline: predict mean of training target\n",
    "naive_pred = np.full(len(y_test), y_train.mean())\n",
    "\n",
    "# Per-regime naive: predict mean of training target within each regime\n",
    "regime_naive_pred = np.zeros(len(y_test))\n",
    "for regime in regime_order:\n",
    "    train_regime_mask = train_meta['regime'].values == regime\n",
    "    test_regime_mask = test_meta['regime'].values == regime\n",
    "    if train_regime_mask.any() and test_regime_mask.any():\n",
    "        regime_mean = y_train[train_regime_mask].mean()\n",
    "        regime_naive_pred[test_regime_mask] = regime_mean\n",
    "# Fallback for unmatched\n",
    "unmatched = regime_naive_pred == 0\n",
    "if unmatched.any():\n",
    "    regime_naive_pred[unmatched] = y_train.mean()\n",
    "\n",
    "# Compile all test results\n",
    "comparison = {\n",
    "    'Naive Mean': naive_pred,\n",
    "    'Regime-Naive Mean': regime_naive_pred,\n",
    "    'Global XGBoost': global_test_pred,\n",
    "    'Regime-Conditioned XGBoost': regime_test_pred,\n",
    "}\n",
    "\n",
    "print(f'{\"Model\":<32s} {\"Test RMSE\":>10s} {\"Test MAE\":>10s} {\"Test R²\":>10s}')\n",
    "print('-' * 65)\n",
    "for name, preds in comparison.items():\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f'{name:<32s} {rmse:>10.6f} {mae:>10.6f} {r2:>10.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "model_names = list(comparison.keys())\n",
    "rmses = [np.sqrt(mean_squared_error(y_test, p)) for p in comparison.values()]\n",
    "r2s = [r2_score(y_test, p) for p in comparison.values()]\n",
    "\n",
    "# RMSE\n",
    "bar_colors = ['lightgray', 'lightblue', 'steelblue', 'darkorange']\n",
    "axes[0].barh(model_names, rmses, color=bar_colors, alpha=0.8)\n",
    "axes[0].set_xlabel('Test RMSE (lower is better)')\n",
    "axes[0].set_title('RMSE Comparison')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# R²\n",
    "axes[1].barh(model_names, r2s, color=bar_colors, alpha=0.8)\n",
    "axes[1].axvline(0, color='red', linestyle='--', lw=1, label='R²=0 (mean baseline)')\n",
    "axes[1].set_xlabel('Test R² (higher is better)')\n",
    "axes[1].set_title('R² Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle('Global vs Regime-Conditioned Models', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-regime breakdown on test set\n",
    "print(f'{\"Regime\":<10s} {\"N\":>5s}  {\"Global RMSE\":>12s} {\"Global R²\":>10s}  {\"Regime RMSE\":>12s} {\"Regime R²\":>10s}  {\"Δ R²\":>8s}')\n",
    "print('-' * 80)\n",
    "\n",
    "for regime in regime_order:\n",
    "    mask = test_meta['regime'].values == regime\n",
    "    if not mask.any():\n",
    "        continue\n",
    "    y_r = y_test[mask]\n",
    "    g_pred = global_test_pred[mask]\n",
    "    r_pred = regime_test_pred[mask]\n",
    "\n",
    "    g_rmse = np.sqrt(mean_squared_error(y_r, g_pred))\n",
    "    g_r2 = r2_score(y_r, g_pred)\n",
    "    r_rmse = np.sqrt(mean_squared_error(y_r, r_pred))\n",
    "    r_r2 = r2_score(y_r, r_pred)\n",
    "    delta = r_r2 - g_r2\n",
    "\n",
    "    print(f'{regime:<10s} {mask.sum():>5d}  {g_rmse:>12.6f} {g_r2:>10.6f}  {r_rmse:>12.6f} {r_r2:>10.6f}  {delta:>+8.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance per Regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regime_with_models = [r for r in regime_order if r in regime_models and regime_models[r] is not global_model]\n",
    "\n",
    "if len(regime_with_models) > 0:\n",
    "    fig, axes = plt.subplots(1, len(regime_with_models),\n",
    "                             figsize=(7 * len(regime_with_models), max(6, len(common_features) * 0.35)))\n",
    "    if len(regime_with_models) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, regime in zip(axes, regime_with_models):\n",
    "        model = regime_models[regime]\n",
    "        imp = model.feature_importances_\n",
    "        imp_df = pd.DataFrame({'Feature': common_features, 'Importance': imp}).sort_values('Importance', ascending=True)\n",
    "        ax.barh(range(len(imp_df)), imp_df['Importance'], color=colors[regime], alpha=0.7)\n",
    "        ax.set_yticks(range(len(imp_df)))\n",
    "        ax.set_yticklabels(imp_df['Feature'])\n",
    "        ax.set_xlabel('Importance')\n",
    "        ax.set_title(f'{regime.upper()} Regime')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    plt.suptitle('Feature Importance per Regime', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No dedicated regime models were trained (all fell back to global model).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save regime models\n",
    "for regime, model in regime_models.items():\n",
    "    model_path = MODEL_DIR / f'regime_model_{regime}.pkl'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f'✓ Saved {regime} model to {model_path}')\n",
    "\n",
    "# Save comparison metadata\n",
    "regime_meta = {\n",
    "    'target': TARGET,\n",
    "    'features': common_features,\n",
    "    'regimes': regime_order,\n",
    "    'comparison': {}\n",
    "}\n",
    "for name, preds in comparison.items():\n",
    "    regime_meta['comparison'][name] = {\n",
    "        'test_rmse': float(np.sqrt(mean_squared_error(y_test, preds))),\n",
    "        'test_mae': float(mean_absolute_error(y_test, preds)),\n",
    "        'test_r2': float(r2_score(y_test, preds)),\n",
    "    }\n",
    "\n",
    "meta_path = MODEL_DIR / 'regime_comparison_metadata.json'\n",
    "with open(meta_path, 'w') as f:\n",
    "    json.dump(regime_meta, f, indent=2)\n",
    "print(f'\\n✓ Comparison metadata saved to {meta_path}')\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print('Regime Comparison Summary')\n",
    "print(f'{\"=\"*60}')\n",
    "for name, metrics in regime_meta['comparison'].items():\n",
    "    print(f'  {name:<32s}  R²={metrics[\"test_r2\"]:+.6f}')\n",
    "print(f'{\"=\"*60}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
